{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/01 18:06:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb54bacc-b52a-4c25-93d2-2ba0f61de9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1298818-83f6-444b-b8a0-4be5b16fd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 18:09:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.format(\"parquet\").save(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d1ded3-ed8a-4e39-94cb-dd3a3272af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abea48b5-e012-4ae2-a53a-e40350f94e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7c34-f0c5-4dab-bffb-31262db80029",
   "metadata": {},
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a96e33-dab7-4a47-896f-ac8e76062421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [Call_Type#231]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/sf.parquet]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#231]\n",
      "Keys [1]: [Call_Type#231]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#347L]\n",
      "Results [2]: [Call_Type#231, count#348L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [Call_Type#231, count#348L]\n",
      "Arguments: hashpartitioning(Call_Type#231, 200), ENSURE_REQUIREMENTS, [plan_id=65]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [Call_Type#231, count#348L]\n",
      "Keys [1]: [Call_Type#231]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#343L]\n",
      "Results [2]: [Call_Type#231, count(1)#343L AS count(1)#344L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#231, count(1)#344L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM calls\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729b210d-00fb-4c7f-8265-af22aaa6a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 18:25:29 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/01 18:25:29 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/01 18:25:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/01 18:25:35 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.21.0.5\n",
      "23/11/01 18:25:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "23/11/01 18:26:06 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/01 18:26:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/01 18:26:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/01 18:26:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/01 18:26:07 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# would work without sampling, just using it to make it faster\n",
    "(spark.table(\"calls\")\n",
    " .sample(True, 0.01)\n",
    " .write.bucketBy(10, \"Call_Type\")\n",
    " .saveAsTable(\"call_by_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c547a3c-9fe4-4b3f-89c6-2e4118223373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- HashAggregate (3)\n",
      "   +- HashAggregate (2)\n",
      "      +- Scan parquet spark_catalog.default.call_by_type (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.call_by_type\n",
      "Output [1]: [Call_Type#528]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/call_by_type]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#528]\n",
      "Keys [1]: [Call_Type#528]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#570L]\n",
      "Results [2]: [Call_Type#528, count#571L]\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [Call_Type#528, count#571L]\n",
      "Keys [1]: [Call_Type#528]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#566L]\n",
      "Results [2]: [Call_Type#528, count(1)#566L AS count(1)#567L]\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#528, count(1)#567L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM call_by_type\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f3c1-585c-46bf-b65b-ea2c753cf97c",
   "metadata": {},
   "source": [
    "# JOIN Algorithms (for a single machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f52615ae-a01a-4069-9ed2-4f141744cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind_id, color\n",
    "fruits = [\n",
    "    (\"B\", \"Yellow\"),\n",
    "    (\"A\", \"Green\"),\n",
    "    (\"C\", \"Orange\"),\n",
    "    (\"A\", \"Red\"),\n",
    "    (\"C\", \"Purple\"),\n",
    "    (\"B\", \"Green\")\n",
    "]\n",
    "\n",
    "# kind_id, name (assume no duplicate kind_id's)\n",
    "kinds = [\n",
    "    (\"A\", \"Apple\"),\n",
    "    (\"B\", \"Banana\"),\n",
    "    (\"C\", \"Carrot\")\n",
    "]\n",
    "\n",
    "# GOAL: print Yellow Banana, Green Apple, etc (any order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abda8b07-34ef-4973-9744-70edd05be001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Apple', 'B': 'Banana', 'C': 'Carrot'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hash join\n",
    "kind_lookup = dict(kinds)\n",
    "kind_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cb0d473-45c4-4fa9-a602-361294d38525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Banana\n",
      "Green Apple\n",
      "Orange Carrot\n",
      "Red Apple\n",
      "Purple Carrot\n",
      "Green Banana\n"
     ]
    }
   ],
   "source": [
    "for kind_id, color in fruits:\n",
    "    print(color, kind_lookup[kind_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ce4ae74-4c34-4368-b47b-3c7d4ae415fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e15710b6-5d4d-49dc-b1e7-afc0c44feeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Green'),\n",
       " ('A', 'Red'),\n",
       " ('B', 'Green'),\n",
       " ('B', 'Yellow'),\n",
       " ('C', 'Orange'),\n",
       " ('C', 'Purple')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits.sort()\n",
    "kinds.sort()\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ecc7c8-5ebc-4d7e-b0a4-a30e3b3b136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Apple'), ('B', 'Banana'), ('C', 'Carrot')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c80b3c45-99ac-4187-a56a-509e30bfdc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Apple\n",
      "Red Apple\n",
      "Green Banana\n",
      "Yellow Banana\n",
      "Orange Carrot\n",
      "Purple Carrot\n"
     ]
    }
   ],
   "source": [
    "fruit_idx = 0\n",
    "for kind_id, food_name in kinds:\n",
    "    while fruit_idx < len(fruits):\n",
    "        if fruits[fruit_idx][0] > kind_id:\n",
    "            break\n",
    "        elif fruits[fruit_idx][0] == kind_id:\n",
    "            print(fruits[fruit_idx][1], food_name)\n",
    "        fruit_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c176496-3f43-4c1e-863e-afd3c6a9f71a",
   "metadata": {},
   "source": [
    "# Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7154e0dc-cc7a-45fa-ad8e-32f85aae1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1f5fc3a-5866-42c9-aaea-50e937a3fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e842274a-33ae-4d44-9b2d-a72b7661caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "| x1| x2|                  y|\n",
      "+---+---+-------------------+\n",
      "|0.0|0.0| 0.5648266803421843|\n",
      "|0.0|2.0| 2.0939332656644014|\n",
      "|0.0|2.0|  2.994013520808395|\n",
      "|1.0|0.0| 1.5516098432369945|\n",
      "|2.0|0.0|  2.285419378010519|\n",
      "|3.0|0.0|   3.94722988657639|\n",
      "|5.0|0.0|  5.117541828375912|\n",
      "|5.0|2.0| 7.3723615588151405|\n",
      "|5.0|2.0| 7.6335844378293904|\n",
      "|6.0|1.0|  7.209392324924814|\n",
      "|6.0|1.0|  7.309871628543936|\n",
      "|6.0|2.0|  8.325226894959687|\n",
      "|8.0|0.0|   8.65686179465806|\n",
      "|9.0|0.0|  9.335490381175925|\n",
      "|9.0|0.0|  9.930572957740925|\n",
      "|9.0|1.0| 10.627217306329767|\n",
      "|9.0|2.0| 11.480261333812567|\n",
      "|0.0|0.0|0.21694709548391078|\n",
      "|1.0|1.0|  2.061167689108809|\n",
      "|1.0|1.0|  2.471207033151983|\n",
      "+---+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not truly deterministic overall, just at the partition level\n",
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8ab1841-8026-43be-8e0d-fdb29b086d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1142a7fb-c4af-46d1-a884-d3a43d54543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99e3cb3-6eea-4366-838b-ddfe7269be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68, 32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f73eb885-a23d-4242-abd8-892e3823997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "# DecisionTreeRegressor: unfit model\n",
    "# DecisionTreeRegressionModel: fitted model\n",
    "# In Spark, names ending in \"Model\" are the fitted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f5966d-5f01-41db-9a6b-53eaa1c049c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS need a vector column\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"x1\", labelCol=\"y\")\n",
    "# dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "864c02f5-d529-48ea-8988-a82a71a128da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0889870-0c9a-4e86-98af-d48061ce16fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+---------+\n",
      "| x1| x2|                 y| features|\n",
      "+---+---+------------------+---------+\n",
      "|0.0|0.0|0.5415786559153639|(2,[],[])|\n",
      "|0.0|1.0| 1.518929039270323|[0.0,1.0]|\n",
      "|0.0|1.0|1.7877870719920037|[0.0,1.0]|\n",
      "|1.0|0.0|1.1368566307892398|[1.0,0.0]|\n",
      "|2.0|0.0|2.9836589427882205|[2.0,0.0]|\n",
      "|2.0|2.0| 4.214838018715973|[2.0,2.0]|\n",
      "|2.0|2.0| 4.787701540320342|[2.0,2.0]|\n",
      "|2.0|2.0| 4.915889037042399|[2.0,2.0]|\n",
      "|3.0|0.0|3.0305886739745196|[3.0,0.0]|\n",
      "|3.0|0.0| 3.442558020574335|[3.0,0.0]|\n",
      "|3.0|0.0| 3.967805061470494|[3.0,0.0]|\n",
      "|3.0|1.0| 4.755412091033398|[3.0,1.0]|\n",
      "|3.0|2.0| 5.149914759956805|[3.0,2.0]|\n",
      "|4.0|0.0|  4.40652818239079|[4.0,0.0]|\n",
      "|4.0|0.0|4.5872573244424775|[4.0,0.0]|\n",
      "|4.0|1.0| 5.773329264364453|[4.0,1.0]|\n",
      "|4.0|1.0| 5.996211493339068|[4.0,1.0]|\n",
      "|5.0|0.0| 5.015153196328459|[5.0,0.0]|\n",
      "|5.0|0.0| 5.115777378498888|[5.0,0.0]|\n",
      "|5.0|1.0| 6.004784812788598|[5.0,1.0]|\n",
      "+---+---+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "va.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3947e68f-492a-4e2d-960f-3bb9cfe66bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b27a4fba-6010-473f-ac35-e89acbeeb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.regression.DecisionTreeRegressor,\n",
       " pyspark.ml.regression.DecisionTreeRegressionModel)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt), type(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
