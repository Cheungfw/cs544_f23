{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 18:15:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb54bacc-b52a-4c25-93d2-2ba0f61de9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1298818-83f6-444b-b8a0-4be5b16fd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d1ded3-ed8a-4e39-94cb-dd3a3272af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abea48b5-e012-4ae2-a53a-e40350f94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7c34-f0c5-4dab-bffb-31262db80029",
   "metadata": {},
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a96e33-dab7-4a47-896f-ac8e76062421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [Call_Type#301]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/sf.parquet]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#301]\n",
      "Keys [1]: [Call_Type#301]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#372L]\n",
      "Results [2]: [Call_Type#301, count#373L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [Call_Type#301, count#373L]\n",
      "Arguments: hashpartitioning(Call_Type#301, 200), ENSURE_REQUIREMENTS, [plan_id=69]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [Call_Type#301, count#373L]\n",
      "Keys [1]: [Call_Type#301]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#368L]\n",
      "Results [2]: [Call_Type#301, count(1)#368L AS count(1)#369L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#301, count(1)#369L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM calls\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729b210d-00fb-4c7f-8265-af22aaa6a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 18:18:09 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/03 18:18:09 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/03 18:18:14 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/03 18:18:14 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.26.0.2\n",
      "23/11/03 18:18:15 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/11/03 18:18:49 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/03 18:18:49 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/03 18:18:49 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/03 18:18:49 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# would work without sampling, just using it to make it faster\n",
    "(spark.table(\"calls\")\n",
    " .sample(True, 0.01)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .bucketBy(10, \"Call_Type\")\n",
    " .saveAsTable(\"call_by_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c547a3c-9fe4-4b3f-89c6-2e4118223373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- HashAggregate (3)\n",
      "   +- HashAggregate (2)\n",
      "      +- Scan parquet spark_catalog.default.call_by_type (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.call_by_type\n",
      "Output [1]: [Call_Type#518]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/call_by_type]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#518]\n",
      "Keys [1]: [Call_Type#518]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#554L]\n",
      "Results [2]: [Call_Type#518, count#555L]\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [Call_Type#518, count#555L]\n",
      "Keys [1]: [Call_Type#518]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#514L]\n",
      "Results [2]: [Call_Type#518, count(1)#514L AS count(1)#550L]\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#518, count(1)#550L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM call_by_type\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f3c1-585c-46bf-b65b-ea2c753cf97c",
   "metadata": {},
   "source": [
    "# JOIN Algorithms (for a single machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52615ae-a01a-4069-9ed2-4f141744cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind_id, color\n",
    "fruits = [\n",
    "    (\"B\", \"Yellow\"),\n",
    "    (\"A\", \"Green\"),\n",
    "    (\"C\", \"Orange\"),\n",
    "    (\"A\", \"Red\"),\n",
    "    (\"C\", \"Purple\"),\n",
    "    (\"B\", \"Green\")\n",
    "]\n",
    "\n",
    "# kind_id, name (assume no duplicate kind_id's)\n",
    "kinds = [\n",
    "    (\"A\", \"Apple\"),\n",
    "    (\"B\", \"Banana\"),\n",
    "    (\"C\", \"Carrot\")\n",
    "]\n",
    "\n",
    "# GOAL: print Yellow Banana, Green Apple, etc (any order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abda8b07-34ef-4973-9744-70edd05be001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Apple', 'B': 'Banana', 'C': 'Carrot'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hash join\n",
    "kind_lookup = dict(kinds)\n",
    "kind_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb0d473-45c4-4fa9-a602-361294d38525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Banana\n",
      "Green Apple\n",
      "Orange Carrot\n",
      "Red Apple\n",
      "Purple Carrot\n",
      "Green Banana\n"
     ]
    }
   ],
   "source": [
    "for kind_id, color in fruits:\n",
    "    print(color, kind_lookup[kind_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce4ae74-4c34-4368-b47b-3c7d4ae415fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15710b6-5d4d-49dc-b1e7-afc0c44feeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Green'),\n",
       " ('A', 'Red'),\n",
       " ('B', 'Green'),\n",
       " ('B', 'Yellow'),\n",
       " ('C', 'Orange'),\n",
       " ('C', 'Purple')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits.sort()\n",
    "kinds.sort()\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ecc7c8-5ebc-4d7e-b0a4-a30e3b3b136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Apple'), ('B', 'Banana'), ('C', 'Carrot')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c80b3c45-99ac-4187-a56a-509e30bfdc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Apple\n",
      "Red Apple\n",
      "Green Banana\n",
      "Yellow Banana\n",
      "Orange Carrot\n",
      "Purple Carrot\n"
     ]
    }
   ],
   "source": [
    "fruit_idx = 0\n",
    "for kind_id, food_name in kinds:\n",
    "    while fruit_idx < len(fruits):\n",
    "        if fruits[fruit_idx][0] > kind_id:\n",
    "            break\n",
    "        elif fruits[fruit_idx][0] == kind_id:\n",
    "            print(fruits[fruit_idx][1], food_name)\n",
    "        fruit_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c176496-3f43-4c1e-863e-afd3c6a9f71a",
   "metadata": {},
   "source": [
    "# Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7154e0dc-cc7a-45fa-ad8e-32f85aae1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f5fc3a-5866-42c9-aaea-50e937a3fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e842274a-33ae-4d44-9b2d-a72b7661caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "| x1| x2|                 y|\n",
      "+---+---+------------------+\n",
      "|0.0|0.0|0.9158805831109543|\n",
      "|1.0|0.0|1.1401461642969826|\n",
      "|1.0|2.0|3.6487009924055704|\n",
      "|2.0|0.0|2.0717519473221353|\n",
      "|2.0|2.0| 4.546826654000136|\n",
      "|4.0|0.0|4.5906962786996255|\n",
      "|5.0|1.0| 6.138227466126343|\n",
      "|5.0|2.0| 7.546845934732009|\n",
      "|6.0|0.0| 6.042072999619121|\n",
      "|6.0|1.0| 7.116403353920589|\n",
      "|6.0|2.0| 8.118565731071259|\n",
      "|6.0|2.0| 8.246993737593817|\n",
      "|8.0|0.0| 8.946340368002927|\n",
      "|9.0|1.0|10.575857248651747|\n",
      "|9.0|2.0|11.097887594073411|\n",
      "|9.0|2.0|11.625428140472932|\n",
      "|9.0|2.0|11.830727541097351|\n",
      "|0.0|0.0|0.5661008051409427|\n",
      "|2.0|2.0| 4.017432291320147|\n",
      "|2.0|2.0| 4.108159563044744|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# not truly deterministic overall, just at the partition level\n",
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8ab1841-8026-43be-8e0d-fdb29b086d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1142a7fb-c4af-46d1-a884-d3a43d54543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c99e3cb3-6eea-4366-838b-ddfe7269be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f73eb885-a23d-4242-abd8-892e3823997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "# DecisionTreeRegressor: unfit model\n",
    "# DecisionTreeRegressionModel: fitted model\n",
    "# In Spark, names ending in \"Model\" are the fitted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f5966d-5f01-41db-9a6b-53eaa1c049c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS need a vector column\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"x1\", labelCol=\"y\")\n",
    "# dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "864c02f5-d529-48ea-8988-a82a71a128da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0889870-0c9a-4e86-98af-d48061ce16fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+---------+\n",
      "| x1| x2|                 y| features|\n",
      "+---+---+------------------+---------+\n",
      "|0.0|0.0|0.9199799858102046|(2,[],[])|\n",
      "|0.0|1.0|1.9237899890234949|[0.0,1.0]|\n",
      "|0.0|2.0|2.4115514968030958|[0.0,2.0]|\n",
      "|0.0|2.0|2.5050748072620093|[0.0,2.0]|\n",
      "|1.0|0.0|1.7061313609343225|[1.0,0.0]|\n",
      "|1.0|2.0| 3.333662686499369|[1.0,2.0]|\n",
      "|2.0|0.0| 2.003528532155789|[2.0,0.0]|\n",
      "|2.0|0.0|2.2192665598861536|[2.0,0.0]|\n",
      "|2.0|0.0| 2.712624351595545|[2.0,0.0]|\n",
      "|2.0|1.0| 3.222200036760564|[2.0,1.0]|\n",
      "|2.0|2.0| 4.643240011769639|[2.0,2.0]|\n",
      "|2.0|2.0| 4.655538264429331|[2.0,2.0]|\n",
      "|2.0|2.0|4.6807817806409115|[2.0,2.0]|\n",
      "|3.0|2.0|5.6720218292104425|[3.0,2.0]|\n",
      "|4.0|0.0| 4.343506601500501|[4.0,0.0]|\n",
      "|5.0|1.0| 6.560117748527708|[5.0,1.0]|\n",
      "|5.0|1.0| 6.719313708593002|[5.0,1.0]|\n",
      "|5.0|1.0| 6.730630186118628|[5.0,1.0]|\n",
      "|5.0|1.0| 6.999800566404603|[5.0,1.0]|\n",
      "|5.0|2.0| 7.460433637573166|[5.0,2.0]|\n",
      "+---+---+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "va.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3947e68f-492a-4e2d-960f-3bb9cfe66bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 18:19:19 WARN BlockManager: Asked to remove block broadcast_31_piece0, which does not exist\n",
      "23/11/03 18:19:20 WARN BlockManager: Asked to remove block broadcast_31, which does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b27a4fba-6010-473f-ac35-e89acbeeb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.regression.DecisionTreeRegressor,\n",
       " pyspark.ml.regression.DecisionTreeRegressionModel)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06daf624-bbe1-47a8-9ab9-484d87937a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "# unfit: Pipeline\n",
    "# fitted: PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83b2929-ad7f-4ae6-b22e-44461dddf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[va, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f096cdd5-a7e3-481b-95ed-ffdf7918d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34395cf-46c3-4ee4-9e0e-824ad1a50e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.pipeline.Pipeline, pyspark.ml.pipeline.PipelineModel)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43512ff7-3a9e-4760-aafd-225a89acbc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_73c04ff51a69, depth=5, numNodes=47, numFeatures=2\n",
      "  If (feature 0 <= 4.5)\n",
      "   If (feature 0 <= 1.5)\n",
      "    If (feature 1 <= 1.5)\n",
      "     If (feature 1 <= 0.5)\n",
      "      If (feature 0 <= 0.5)\n",
      "       Predict: 0.7800744764970607\n",
      "      Else (feature 0 > 0.5)\n",
      "       Predict: 1.706131360934322\n",
      "     Else (feature 1 > 0.5)\n",
      "      Predict: 1.6506517487404657\n",
      "    Else (feature 1 > 1.5)\n",
      "     If (feature 0 <= 0.5)\n",
      "      Predict: 2.37192942461925\n",
      "     Else (feature 0 > 0.5)\n",
      "      Predict: 3.333662686499369\n",
      "   Else (feature 0 > 1.5)\n",
      "    If (feature 0 <= 2.5)\n",
      "     If (feature 1 <= 0.5)\n",
      "      Predict: 2.311806481212496\n",
      "     Else (feature 1 > 0.5)\n",
      "      If (feature 1 <= 1.5)\n",
      "       Predict: 3.598877925615361\n",
      "      Else (feature 1 > 1.5)\n",
      "       Predict: 4.585099468466768\n",
      "    Else (feature 0 > 2.5)\n",
      "     If (feature 1 <= 1.5)\n",
      "      If (feature 1 <= 0.5)\n",
      "       Predict: 4.4191793805811805\n",
      "      Else (feature 1 > 0.5)\n",
      "       Predict: 5.358576016281355\n",
      "     Else (feature 1 > 1.5)\n",
      "      If (feature 0 <= 3.5)\n",
      "       Predict: 5.792059324228026\n",
      "      Else (feature 0 > 3.5)\n",
      "       Predict: 6.6741159095042635\n",
      "  Else (feature 0 > 4.5)\n",
      "   If (feature 0 <= 6.5)\n",
      "    If (feature 1 <= 1.5)\n",
      "     If (feature 0 <= 5.5)\n",
      "      Predict: 6.681592758647609\n",
      "     Else (feature 0 > 5.5)\n",
      "      If (feature 1 <= 0.5)\n",
      "       Predict: 6.796495279798175\n",
      "      Else (feature 1 > 0.5)\n",
      "       Predict: 7.363130186403168\n",
      "    Else (feature 1 > 1.5)\n",
      "     If (feature 0 <= 5.5)\n",
      "      Predict: 7.460433637573166\n",
      "     Else (feature 0 > 5.5)\n",
      "      Predict: 8.351858905012657\n",
      "   Else (feature 0 > 6.5)\n",
      "    If (feature 1 <= 0.5)\n",
      "     If (feature 0 <= 8.5)\n",
      "      If (feature 0 <= 7.5)\n",
      "       Predict: 7.565273901399568\n",
      "      Else (feature 0 > 7.5)\n",
      "       Predict: 8.060057626466202\n",
      "     Else (feature 0 > 8.5)\n",
      "      Predict: 9.360647300619291\n",
      "    Else (feature 1 > 0.5)\n",
      "     If (feature 0 <= 7.5)\n",
      "      If (feature 1 <= 1.5)\n",
      "       Predict: 8.896133847712079\n",
      "      Else (feature 1 > 1.5)\n",
      "       Predict: 9.689394106501865\n",
      "     Else (feature 0 > 7.5)\n",
      "      If (feature 0 <= 8.5)\n",
      "       Predict: 10.304131902324803\n",
      "      Else (feature 0 > 8.5)\n",
      "       Predict: 11.145289250702923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[1].toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdb71c7e-e7d2-4d82-a35c-a5e9f65825cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b6bc359-0800-4690-82e1-5625d28bd5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-03 18:25 hdfs://nn:9000/model/stages/0_VectorAssembler_fb04de5c9f7b\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-03 18:25 hdfs://nn:9000/model/stages/1_DecisionTreeRegressor_73c04ff51a69\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/model/stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f1d928e-6a9c-4331-b787-1f86eca4b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bd76793-d180-43b0-837e-caa0d94e1a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57bd176d-ad04-42aa-8e45-582d14d0f1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double, features: vector, prediction: double]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6445528e-f1dd-4bb0-b264-8b01199db710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+---------+------------------+\n",
      "| x1| x2|                 y| features|        prediction|\n",
      "+---+---+------------------+---------+------------------+\n",
      "|0.0|0.0|0.7541983929002224|(2,[],[])|0.7800744764970607|\n",
      "|0.0|2.0|2.0895679475224442|[0.0,2.0]|  2.37192942461925|\n",
      "|2.0|0.0|2.8887728263009396|[2.0,0.0]| 2.311806481212496|\n",
      "|2.0|1.0| 3.108342200029332|[2.0,1.0]| 3.598877925615361|\n",
      "|3.0|1.0| 4.406243872036037|[3.0,1.0]| 5.358576016281355|\n",
      "|4.0|1.0| 5.995543550470651|[4.0,1.0]| 5.358576016281355|\n",
      "|5.0|1.0| 6.085087714599171|[5.0,1.0]| 6.681592758647609|\n",
      "|7.0|0.0| 7.776839011240047|[7.0,0.0]| 7.565273901399568|\n",
      "|7.0|1.0| 8.288747701615705|[7.0,1.0]| 8.896133847712079|\n",
      "|7.0|2.0| 9.186696500928358|[7.0,2.0]| 9.689394106501865|\n",
      "|7.0|2.0|  9.61693890608781|[7.0,2.0]| 9.689394106501865|\n",
      "|7.0|2.0| 9.704914189877377|[7.0,2.0]| 9.689394106501865|\n",
      "|9.0|1.0|10.933586729559503|[9.0,1.0]|11.145289250702923|\n",
      "|9.0|2.0|11.116121782661093|[9.0,2.0]|11.145289250702923|\n",
      "|9.0|2.0|11.428572740379153|[9.0,2.0]|11.145289250702923|\n",
      "|9.0|2.0|11.606732099502965|[9.0,2.0]|11.145289250702923|\n",
      "|9.0|2.0|11.955215304027655|[9.0,2.0]|11.145289250702923|\n",
      "|0.0|0.0|0.5106550677158106|(2,[],[])|0.7800744764970607|\n",
      "|0.0|2.0| 2.523084568755169|[0.0,2.0]|  2.37192942461925|\n",
      "|0.0|2.0|2.9538143936081203|[0.0,2.0]|  2.37192942461925|\n",
      "+---+---+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66b6f930-b987-4960-9ee9-ef3d219d7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1a5b2b8-bc2c-4dfc-8f3f-03076afb4b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionEvaluator_e367dae4265a"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2score = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"y\", metricName=\"r2\")\n",
    "r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f46304a-db15-473e-a60a-aa46d06352eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9826104185724672"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2score.evaluate(model.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
