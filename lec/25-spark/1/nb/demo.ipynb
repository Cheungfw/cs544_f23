{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/01 14:25:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb54bacc-b52a-4c25-93d2-2ba0f61de9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1298818-83f6-444b-b8a0-4be5b16fd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d1ded3-ed8a-4e39-94cb-dd3a3272af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abea48b5-e012-4ae2-a53a-e40350f94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7c34-f0c5-4dab-bffb-31262db80029",
   "metadata": {},
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08478e06-6544-4b23-9ff7-308db984da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [Call_Type#301]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/sf.parquet]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#301]\n",
      "Keys [1]: [Call_Type#301]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#460L]\n",
      "Results [2]: [Call_Type#301, count#461L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [Call_Type#301, count#461L]\n",
      "Arguments: hashpartitioning(Call_Type#301, 200), ENSURE_REQUIREMENTS, [plan_id=94]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [Call_Type#301, count#461L]\n",
      "Keys [1]: [Call_Type#301]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#456L]\n",
      "Results [2]: [Call_Type#301, count(1)#456L AS count(1)#457L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#301, count(1)#457L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*) FROM calls GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e444e659-cb81-4f7d-b80c-526086ec63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 15:00:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/01 15:00:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/01 15:00:13 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/01 15:00:13 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.20.0.5\n",
      "23/11/01 15:00:13 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "23/11/01 15:00:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/01 15:00:52 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/01 15:00:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/01 15:00:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/01 15:00:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# sample is just to make the demo faster, it would work without that\n",
    "(spark.table(\"calls\")\n",
    " .sample(True, 0.01)\n",
    " .repartition(10, \"Call_Type\")\n",
    " .write\n",
    " .bucketBy(10, \"Call_Type\")\n",
    " .saveAsTable(\"calls_by_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947a3415-4b85-4b9a-8058-0f7cc289c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- HashAggregate (3)\n",
      "   +- HashAggregate (2)\n",
      "      +- Scan parquet spark_catalog.default.calls_by_type (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.calls_by_type\n",
      "Output [1]: [Call_Type#641]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/calls_by_type]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#641]\n",
      "Keys [1]: [Call_Type#641]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#677L]\n",
      "Results [2]: [Call_Type#641, count#678L]\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [Call_Type#641, count#678L]\n",
      "Keys [1]: [Call_Type#641]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#637L]\n",
      "Results [2]: [Call_Type#641, count(1)#637L AS count(1)#673L]\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#641, count(1)#673L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM calls_by_type\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84b577-d547-4b0d-8d0d-073866626e68",
   "metadata": {},
   "source": [
    "# JOIN (single machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceaa419e-0a03-429d-baec-c69940149f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind_id, color\n",
    "fruits = [\n",
    "    (\"B\", \"Yellow\"),\n",
    "    (\"A\", \"Green\"),\n",
    "    (\"C\", \"Orange\"),\n",
    "    (\"A\", \"Red\"),\n",
    "    (\"C\", \"Purple\"),\n",
    "    (\"B\", \"Green\")\n",
    "]\n",
    "\n",
    "# kind_id, name (assume no duplicate kind_id's)\n",
    "kinds = [\n",
    "    (\"A\", \"Apple\"),\n",
    "    (\"B\", \"Banana\"),\n",
    "    (\"C\", \"Carrot\")\n",
    "]\n",
    "\n",
    "# GOAL: print Yellow Banana, Green Apple, etc (any order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768db798-d041-427c-8e17-d6be512da884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Apple', 'B': 'Banana', 'C': 'Carrot'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hash Join\n",
    "kind_lookup = dict(kinds)\n",
    "kind_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d2c85b1-b7cf-4296-8b4d-a5eb493ed711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Banana\n",
      "Green Apple\n",
      "Orange Carrot\n",
      "Red Apple\n",
      "Purple Carrot\n",
      "Green Banana\n"
     ]
    }
   ],
   "source": [
    "for kind_id, color in fruits:\n",
    "    print(color, kind_lookup[kind_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d8b01-c137-40ff-8a59-1a1202b9b97c",
   "metadata": {},
   "source": [
    "# Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9d43e08-66fc-4d69-a48d-1b300ee1d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds.sort()\n",
    "fruits.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c85fc29c-101b-4105-b8db-5abbb506b71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Apple'), ('B', 'Banana'), ('C', 'Carrot')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41bc2d9c-5ca0-4fc8-b357-f72216523dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Green'),\n",
       " ('A', 'Red'),\n",
       " ('B', 'Green'),\n",
       " ('B', 'Yellow'),\n",
       " ('C', 'Orange'),\n",
       " ('C', 'Purple')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f479f4c-1e28-4cfc-91a1-22acb009cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Apple\n",
      "Red Apple\n",
      "Green Banana\n",
      "Yellow Banana\n",
      "Orange Carrot\n",
      "Purple Carrot\n"
     ]
    }
   ],
   "source": [
    "fruit_idx = 0\n",
    "for kind_id, fruit_name in kinds:\n",
    "    while fruit_idx < len(fruits):\n",
    "        if kind_id == fruits[fruit_idx][0]:\n",
    "            print(fruits[fruit_idx][1], fruit_name)\n",
    "        elif fruits[fruit_idx][0] > kind_id:\n",
    "            break\n",
    "        fruit_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9294d5-7ea9-4b14-9726-827c61fb1ccc",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1baad34c-58aa-49e1-a9bc-707a5f4029ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))\n",
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8f34f15-e4cf-46df-a35d-57cd7fdd9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "| x1| x2|                 y|\n",
      "+---+---+------------------+\n",
      "|0.0|1.0| 1.264283604109949|\n",
      "|0.0|2.0| 2.094890224170356|\n",
      "|0.0|2.0| 2.873546913558723|\n",
      "|1.0|1.0|2.0277341720023143|\n",
      "|1.0|2.0|3.5553849236676727|\n",
      "|3.0|2.0|5.8380918111023465|\n",
      "|4.0|2.0|6.0396629102184445|\n",
      "|5.0|2.0| 7.908860662294932|\n",
      "|6.0|0.0| 6.900819059812529|\n",
      "|6.0|2.0| 8.770396197908603|\n",
      "|7.0|0.0| 7.494266814295333|\n",
      "|7.0|1.0| 8.752501791927529|\n",
      "|9.0|0.0| 9.285365159223563|\n",
      "|9.0|0.0| 9.780787928310911|\n",
      "|9.0|1.0|10.647385930231051|\n",
      "|9.0|2.0|11.137063529364992|\n",
      "|9.0|2.0|11.361406110699855|\n",
      "|0.0|0.0|0.3542937425216831|\n",
      "|0.0|1.0|1.7711931101036376|\n",
      "|0.0|2.0|2.5348876398044298|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d261617-5525-4465-853f-7cc6a52eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ebbffc4-6f5e-4adf-9d4a-5914b7ef5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88d408af-8687-46e8-b440-d3ec04e8efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68, 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "627d9d99-ed5e-4ddf-944c-e11605da57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "# DecisionTreeRegressor: unfit model\n",
    "# DecisionTreeRegressionModel: fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae2f3a97-b20c-4696-b4c2-cc15b310e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need vectors!\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"x1\", labelCol=\"y\")\n",
    "# dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de1e5dfb-ac26-4f81-bdcd-b505de6651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3d9877c-5464-493f-8851-81f68dce390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_fa9a2b1c3058"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09eb9cd4-f377-49da-b39b-03cb98d34ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8520f61f-8f9e-48dd-86d2-ec3e92f8ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f61d9c2a-791d-4d9c-8b8d-3160489934c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.regression.DecisionTreeRegressor,\n",
       " pyspark.ml.regression.DecisionTreeRegressionModel)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ad01fd9-5f82-42f3-8f98-cf878526d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "# Pipeline: unfit\n",
    "# PipelineModel: fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd5807fd-d9be-4ac4-98f0-e2ca159b0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[va, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1eec0071-759d-4f10-ad24-2e7252e201a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.pipeline.Pipeline, pyspark.ml.pipeline.PipelineModel)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipe.fit(train)\n",
    "type(pipe), type(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
