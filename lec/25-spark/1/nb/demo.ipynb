{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/03 13:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb54bacc-b52a-4c25-93d2-2ba0f61de9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1298818-83f6-444b-b8a0-4be5b16fd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 13:33:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d1ded3-ed8a-4e39-94cb-dd3a3272af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abea48b5-e012-4ae2-a53a-e40350f94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7c34-f0c5-4dab-bffb-31262db80029",
   "metadata": {},
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08478e06-6544-4b23-9ff7-308db984da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [Call_Type#231]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/sf.parquet]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#231]\n",
      "Keys [1]: [Call_Type#231]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#302L]\n",
      "Results [2]: [Call_Type#231, count#303L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [Call_Type#231, count#303L]\n",
      "Arguments: hashpartitioning(Call_Type#231, 200), ENSURE_REQUIREMENTS, [plan_id=52]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [Call_Type#231, count#303L]\n",
      "Keys [1]: [Call_Type#231]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#298L]\n",
      "Results [2]: [Call_Type#231, count(1)#298L AS count(1)#299L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#231, count(1)#299L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*) FROM calls GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e444e659-cb81-4f7d-b80c-526086ec63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/03 13:41:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/11/03 13:41:49 WARN HadoopFSUtils: The directory hdfs://nn:9000/user/hive/warehouse/calls_by_type was not found. Was it deleted very recently?\n",
      "23/11/03 13:41:51 WARN FileUtils: File does not exist: hdfs://nn:9000/user/hive/warehouse/calls_by_type; Force to delete it.\n",
      "23/11/03 13:41:51 ERROR FileUtils: Failed to delete hdfs://nn:9000/user/hive/warehouse/calls_by_type\n",
      "23/11/03 13:42:20 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/03 13:42:20 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/03 13:42:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/03 13:42:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# sample is just to make the demo faster, it would work without that\n",
    "(spark.table(\"calls\")\n",
    " .sample(True, 0.01)\n",
    " .repartition(10, \"Call_Type\")\n",
    " .write\n",
    " .bucketBy(10, \"Call_Type\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"calls_by_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947a3415-4b85-4b9a-8058-0f7cc289c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- HashAggregate (3)\n",
      "   +- HashAggregate (2)\n",
      "      +- Scan parquet spark_catalog.default.calls_by_type (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.calls_by_type\n",
      "Output [1]: [Call_Type#518]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/calls_by_type]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#518]\n",
      "Keys [1]: [Call_Type#518]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#554L]\n",
      "Results [2]: [Call_Type#518, count#555L]\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [Call_Type#518, count#555L]\n",
      "Keys [1]: [Call_Type#518]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#514L]\n",
      "Results [2]: [Call_Type#518, count(1)#514L AS count(1)#550L]\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#518, count(1)#550L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM calls_by_type\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84b577-d547-4b0d-8d0d-073866626e68",
   "metadata": {},
   "source": [
    "# JOIN (single machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaa419e-0a03-429d-baec-c69940149f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind_id, color\n",
    "fruits = [\n",
    "    (\"B\", \"Yellow\"),\n",
    "    (\"A\", \"Green\"),\n",
    "    (\"C\", \"Orange\"),\n",
    "    (\"A\", \"Red\"),\n",
    "    (\"C\", \"Purple\"),\n",
    "    (\"B\", \"Green\")\n",
    "]\n",
    "\n",
    "# kind_id, name (assume no duplicate kind_id's)\n",
    "kinds = [\n",
    "    (\"A\", \"Apple\"),\n",
    "    (\"B\", \"Banana\"),\n",
    "    (\"C\", \"Carrot\")\n",
    "]\n",
    "\n",
    "# GOAL: print Yellow Banana, Green Apple, etc (any order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768db798-d041-427c-8e17-d6be512da884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Apple', 'B': 'Banana', 'C': 'Carrot'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hash Join\n",
    "kind_lookup = dict(kinds)\n",
    "kind_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d2c85b1-b7cf-4296-8b4d-a5eb493ed711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Banana\n",
      "Green Apple\n",
      "Orange Carrot\n",
      "Red Apple\n",
      "Purple Carrot\n",
      "Green Banana\n"
     ]
    }
   ],
   "source": [
    "for kind_id, color in fruits:\n",
    "    print(color, kind_lookup[kind_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d8b01-c137-40ff-8a59-1a1202b9b97c",
   "metadata": {},
   "source": [
    "# Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d43e08-66fc-4d69-a48d-1b300ee1d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds.sort()\n",
    "fruits.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85fc29c-101b-4105-b8db-5abbb506b71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Apple'), ('B', 'Banana'), ('C', 'Carrot')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41bc2d9c-5ca0-4fc8-b357-f72216523dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Green'),\n",
       " ('A', 'Red'),\n",
       " ('B', 'Green'),\n",
       " ('B', 'Yellow'),\n",
       " ('C', 'Orange'),\n",
       " ('C', 'Purple')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f479f4c-1e28-4cfc-91a1-22acb009cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Apple\n",
      "Red Apple\n",
      "Green Banana\n",
      "Yellow Banana\n",
      "Orange Carrot\n",
      "Purple Carrot\n"
     ]
    }
   ],
   "source": [
    "fruit_idx = 0\n",
    "for kind_id, fruit_name in kinds:\n",
    "    while fruit_idx < len(fruits):\n",
    "        if kind_id == fruits[fruit_idx][0]:\n",
    "            print(fruits[fruit_idx][1], fruit_name)\n",
    "        elif fruits[fruit_idx][0] > kind_id:\n",
    "            break\n",
    "        fruit_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9294d5-7ea9-4b14-9726-827c61fb1ccc",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1baad34c-58aa-49e1-a9bc-707a5f4029ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: double, x2: double, y: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))\n",
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f34f15-e4cf-46df-a35d-57cd7fdd9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "| x1| x2|                 y|\n",
      "+---+---+------------------+\n",
      "|0.0|0.0|0.9108395611170045|\n",
      "|1.0|2.0| 3.641410393897231|\n",
      "|1.0|2.0|3.9464489632070117|\n",
      "|2.0|0.0| 2.527813970394922|\n",
      "|2.0|2.0| 4.590305173822881|\n",
      "|4.0|0.0| 4.553553282229819|\n",
      "|5.0|2.0| 7.913702301507376|\n",
      "|6.0|1.0| 7.868747400415531|\n",
      "|6.0|2.0| 8.486328666168749|\n",
      "|7.0|0.0| 7.275548337048227|\n",
      "|7.0|0.0| 7.728987032697484|\n",
      "|7.0|1.0| 8.264617871091119|\n",
      "|8.0|2.0|10.429529795420434|\n",
      "|8.0|2.0|10.802189014971018|\n",
      "|9.0|1.0|10.551584632951641|\n",
      "|9.0|2.0|11.336390999413332|\n",
      "|9.0|2.0|11.944050961335062|\n",
      "|0.0|1.0|1.5202523837201376|\n",
      "|2.0|1.0|3.8331570846415604|\n",
      "|2.0|2.0| 4.559821959933614|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d261617-5525-4465-853f-7cc6a52eb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebbffc4-6f5e-4adf-9d4a-5914b7ef5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d408af-8687-46e8-b440-d3ec04e8efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "627d9d99-ed5e-4ddf-944c-e11605da57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "# DecisionTreeRegressor: unfit model\n",
    "# DecisionTreeRegressionModel: fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae2f3a97-b20c-4696-b4c2-cc15b310e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need vectors!\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"x1\", labelCol=\"y\")\n",
    "# dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de1e5dfb-ac26-4f81-bdcd-b505de6651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d9877c-5464-493f-8851-81f68dce390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_7d8ca541415c"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09eb9cd4-f377-49da-b39b-03cb98d34ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8520f61f-8f9e-48dd-86d2-ec3e92f8ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61d9c2a-791d-4d9c-8b8d-3160489934c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.regression.DecisionTreeRegressor,\n",
       " pyspark.ml.regression.DecisionTreeRegressionModel)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ad01fd9-5f82-42f3-8f98-cf878526d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "# Pipeline: unfit\n",
    "# PipelineModel: fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5807fd-d9be-4ac4-98f0-e2ca159b0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[va, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eec0071-759d-4f10-ad24-2e7252e201a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.ml.pipeline.Pipeline, pyspark.ml.pipeline.PipelineModel)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipe.fit(train)\n",
    "type(pipe), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac917af1-c0cb-4d34-a7c4-89e61575ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7371f0dd-e4a1-4ab8-bfb8-b30e8a9f3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-03 14:59 hdfs://nn:9000/model/stages/0_VectorAssembler_7d8ca541415c\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-03 14:59 hdfs://nn:9000/model/stages/1_DecisionTreeRegressor_93027dcaa5db\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/model/stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52c006ef-366c-4452-a8ff-4b7693cab5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de680af8-d5d5-4f93-a666-8c3da7b8cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "| x1| x2|                 y|\n",
      "+---+---+------------------+\n",
      "|0.0|0.0|0.9108395611170045|\n",
      "|1.0|2.0| 3.641410393897231|\n",
      "|1.0|2.0|3.9464489632070117|\n",
      "|2.0|0.0| 2.527813970394922|\n",
      "|2.0|2.0| 4.590305173822881|\n",
      "|4.0|0.0| 4.553553282229819|\n",
      "|5.0|2.0| 7.913702301507376|\n",
      "|6.0|1.0| 7.868747400415531|\n",
      "|6.0|2.0| 8.486328666168749|\n",
      "|7.0|0.0| 7.275548337048227|\n",
      "|7.0|0.0| 7.728987032697484|\n",
      "|7.0|1.0| 8.264617871091119|\n",
      "|8.0|2.0|10.429529795420434|\n",
      "|8.0|2.0|10.802189014971018|\n",
      "|9.0|1.0|10.551584632951641|\n",
      "|9.0|2.0|11.336390999413332|\n",
      "|9.0|2.0|11.944050961335062|\n",
      "|0.0|1.0|1.5202523837201376|\n",
      "|2.0|1.0|3.8331570846415604|\n",
      "|2.0|2.0| 4.559821959933614|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96d2bbfb-05b2-4eef-b74b-b485d8ba78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+---------+------------------+\n",
      "| x1| x2|                 y| features|        prediction|\n",
      "+---+---+------------------+---------+------------------+\n",
      "|0.0|0.0|0.9108395611170045|(2,[],[])|0.3629253964995967|\n",
      "|1.0|2.0| 3.641410393897231|[1.0,2.0]| 3.078665082659273|\n",
      "|1.0|2.0|3.9464489632070117|[1.0,2.0]| 3.078665082659273|\n",
      "|2.0|0.0| 2.527813970394922|[2.0,0.0]|2.7613042547531155|\n",
      "|2.0|2.0| 4.590305173822881|[2.0,2.0]| 4.959259399838608|\n",
      "|4.0|0.0| 4.553553282229819|[4.0,0.0]| 5.077959347669859|\n",
      "|5.0|2.0| 7.913702301507376|[5.0,2.0]| 7.093386194668144|\n",
      "|6.0|1.0| 7.868747400415531|[6.0,1.0]| 7.661040434961876|\n",
      "|6.0|2.0| 8.486328666168749|[6.0,2.0]| 8.646661072348877|\n",
      "|7.0|0.0| 7.275548337048227|[7.0,0.0]| 7.525592969042154|\n",
      "|7.0|0.0| 7.728987032697484|[7.0,0.0]| 7.525592969042154|\n",
      "|7.0|1.0| 8.264617871091119|[7.0,1.0]| 8.475563904560868|\n",
      "|8.0|2.0|10.429529795420434|[8.0,2.0]|10.598821170877377|\n",
      "|8.0|2.0|10.802189014971018|[8.0,2.0]|10.598821170877377|\n",
      "|9.0|1.0|10.551584632951641|[9.0,1.0]| 9.418042271622454|\n",
      "|9.0|2.0|11.336390999413332|[9.0,2.0]|11.503347686589557|\n",
      "|9.0|2.0|11.944050961335062|[9.0,2.0]|11.503347686589557|\n",
      "|0.0|1.0|1.5202523837201376|[0.0,1.0]|2.1426232679288075|\n",
      "|2.0|1.0|3.8331570846415604|[2.0,1.0]|3.1434715268552127|\n",
      "|2.0|2.0| 4.559821959933614|[2.0,2.0]| 4.959259399838608|\n",
      "+---+---+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df1f09e4-91c6-451c-8346-9f4ebe3c3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfe8e99e-cc89-4fcb-9a8c-e402f7cd11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2score = RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75b33f15-1db9-4ddf-ba92-62568f5e46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9730704358867571"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2score.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c7520ab-6ef8-4cec-a064-895df20dee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_93027dcaa5db, depth=5, numNodes=47, numFeatures=2\n",
      "  If (feature 0 <= 5.5)\n",
      "   If (feature 0 <= 2.5)\n",
      "    If (feature 1 <= 0.5)\n",
      "     If (feature 0 <= 1.5)\n",
      "      If (feature 0 <= 0.5)\n",
      "       Predict: 0.3629253964995967\n",
      "      Else (feature 0 > 0.5)\n",
      "       Predict: 1.395077573815772\n",
      "     Else (feature 0 > 1.5)\n",
      "      Predict: 2.7613042547531155\n",
      "    Else (feature 1 > 0.5)\n",
      "     If (feature 0 <= 1.5)\n",
      "      If (feature 1 <= 1.5)\n",
      "       Predict: 2.1426232679288075\n",
      "      Else (feature 1 > 1.5)\n",
      "       Predict: 3.078665082659273\n",
      "     Else (feature 0 > 1.5)\n",
      "      If (feature 1 <= 1.5)\n",
      "       Predict: 3.1434715268552127\n",
      "      Else (feature 1 > 1.5)\n",
      "       Predict: 4.959259399838608\n",
      "   Else (feature 0 > 2.5)\n",
      "    If (feature 0 <= 3.5)\n",
      "     If (feature 1 <= 0.5)\n",
      "      Predict: 3.5703031985879723\n",
      "     Else (feature 1 > 0.5)\n",
      "      If (feature 1 <= 1.5)\n",
      "       Predict: 4.512185877675951\n",
      "      Else (feature 1 > 1.5)\n",
      "       Predict: 5.204072300321589\n",
      "    Else (feature 0 > 3.5)\n",
      "     If (feature 1 <= 1.5)\n",
      "      If (feature 1 <= 0.5)\n",
      "       Predict: 5.077959347669859\n",
      "      Else (feature 1 > 0.5)\n",
      "       Predict: 5.5319502106266025\n",
      "     Else (feature 1 > 1.5)\n",
      "      If (feature 0 <= 4.5)\n",
      "       Predict: 6.707188413083328\n",
      "      Else (feature 0 > 4.5)\n",
      "       Predict: 7.093386194668144\n",
      "  Else (feature 0 > 5.5)\n",
      "   If (feature 1 <= 1.5)\n",
      "    If (feature 0 <= 6.5)\n",
      "     If (feature 1 <= 0.5)\n",
      "      Predict: 6.343032350792138\n",
      "     Else (feature 1 > 0.5)\n",
      "      Predict: 7.661040434961876\n",
      "    Else (feature 0 > 6.5)\n",
      "     If (feature 0 <= 7.5)\n",
      "      If (feature 1 <= 0.5)\n",
      "       Predict: 7.525592969042154\n",
      "      Else (feature 1 > 0.5)\n",
      "       Predict: 8.475563904560868\n",
      "     Else (feature 0 > 7.5)\n",
      "      If (feature 0 <= 8.5)\n",
      "       Predict: 8.889178468718468\n",
      "      Else (feature 0 > 8.5)\n",
      "       Predict: 9.418042271622454\n",
      "   Else (feature 1 > 1.5)\n",
      "    If (feature 0 <= 7.5)\n",
      "     If (feature 0 <= 6.5)\n",
      "      Predict: 8.646661072348877\n",
      "     Else (feature 0 > 6.5)\n",
      "      Predict: 9.593294437406433\n",
      "    Else (feature 0 > 7.5)\n",
      "     If (feature 0 <= 8.5)\n",
      "      Predict: 10.598821170877377\n",
      "     Else (feature 0 > 8.5)\n",
      "      Predict: 11.503347686589557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[1].toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
