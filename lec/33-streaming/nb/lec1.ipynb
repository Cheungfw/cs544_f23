{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea04583-214c-4547-a346-24211910e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m grpc_tools.protoc -I=. --python_out=. animals.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12accd7e-ea68-4895-9650-24d3e601456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63fe0d9d-e254-44ff-bff6-9c63d51b462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d28414-f67d-476b-8261-21b9defbc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9d8673-f7ec-4c6b-890e-6e1963776b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2c0eb0-15c2-4ec5-9783-d2e355e44d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4327cc6-901f-499c-94a0-cbb273c75b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals\", 4, 1)]) # protobufs\n",
    "except TopicAlreadyExistsError:\n",
    "    pass\n",
    "try:\n",
    "    admin.create_topics([NewTopic(\"animals-json\", 4, 1)]) # json\n",
    "except TopicAlreadyExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861305e1-71d0-4cd1-ac8a-247745bb00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8fc2b41-acf7-424d-b3ff-8de8a485c90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f4698134d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"A\"\n",
    "value = Sighting(beach=key, animal=\"shark\").SerializeToString()\n",
    "producer.send(\"animals\", value, bytes(key, \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b604fccb-08f2-4b20-ab32-a527d5ab5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, threading\n",
    "\n",
    "def animal_producer():\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        value = Sighting(beach=beach, animal=animal).SerializeToString()\n",
    "        producer.send(\"animals\", value, bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "threading.Thread(target=animal_producer).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e888ee4-6e7b-495c-bb6a-160fa1a4926d",
   "metadata": {},
   "source": [
    "# Streaming Group BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb5d3f5-612a-4162-b08e-4ce5e9c0755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "\n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28983c4f-5925-46c2-8c5e-260299154a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97dcd6e6-4903-4718-be97-35a47ead78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TopicPartition(topic='animals', partition=0), TopicPartition(topic='animals', partition=1)]\n",
      "[TopicPartition(topic='animals', partition=2), TopicPartition(topic='animals', partition=3)]\n",
      "[0, 1] {'I': 64, 'C': 46, 'B': 50, 'D': 64, 'E': 46}\n",
      "[2, 3] {'A': 41, 'F': 50, 'G': 48, 'H': 57}\n",
      "[2, 3] {'A': 41, 'F': 51, 'G': 48, 'H': 57}\n",
      "[0, 1] {'I': 64, 'C': 46, 'B': 50, 'D': 64, 'E': 46}\n",
      "[0, 1] {'I': 64, 'C': 46, 'B': 50, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 41, 'F': 51, 'G': 48, 'H': 57}\n",
      "[0, 1] {'I': 64, 'C': 46, 'B': 50, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 41, 'F': 51, 'G': 48, 'H': 57}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 50, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 41, 'F': 51, 'G': 49, 'H': 57}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 50, 'D': 65, 'E': 46}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 51, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 41, 'F': 51, 'G': 49, 'H': 57}\n",
      "[2, 3] {'A': 42, 'F': 51, 'G': 49, 'H': 57}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 51, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 42, 'F': 52, 'G': 49, 'H': 57}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 51, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 42, 'F': 52, 'G': 49, 'H': 58}\n",
      "[0, 1] {'I': 65, 'C': 46, 'B': 51, 'D': 65, 'E': 46}\n",
      "[2, 3] {'A': 42, 'F': 53, 'G': 49, 'H': 58}\n"
     ]
    }
   ],
   "source": [
    "def beach_consumer(parts=[]):\n",
    "    counts = {}  # key=beach, value=count\n",
    "    partitions = [TopicPartition(\"animals\", p) for p in parts]\n",
    "    print(partitions)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign(partitions)\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if not s.beach in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(parts, counts)\n",
    "\n",
    "threading.Thread(target=beach_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80354fec-af78-485c-9dcb-31d3a826a693",
   "metadata": {},
   "source": [
    "# Another Group BY, but not by the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "878c6117-4816-488e-b8bc-5e3458e97f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TopicPartition(topic='animals', partition=0), TopicPartition(topic='animals', partition=1)]\n",
      "[TopicPartition(topic='animals', partition=2), TopicPartition(topic='animals', partition=3)]\n",
      "[0, 1] {'shark': 87, 'turtle': 73, 'dolphin': 77, 'seagull': 84}\n",
      "[2, 3] {'shark': 74, 'turtle': 59, 'dolphin': 58, 'seagull': 52}\n",
      "[0, 1] {'shark': 87, 'turtle': 73, 'dolphin': 77, 'seagull': 85}\n",
      "[2, 3] {'shark': 74, 'turtle': 59, 'dolphin': 58, 'seagull': 52}\n",
      "[0, 1] {'shark': 87, 'turtle': 73, 'dolphin': 77, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 59, 'dolphin': 58, 'seagull': 52}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 77, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 59, 'dolphin': 58, 'seagull': 52}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 59, 'dolphin': 58, 'seagull': 52}\n",
      "[2, 3] {'shark': 74, 'turtle': 60, 'dolphin': 58, 'seagull': 52}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 60, 'dolphin': 58, 'seagull': 52}\n",
      "[2, 3] {'shark': 74, 'turtle': 60, 'dolphin': 58, 'seagull': 53}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 61, 'dolphin': 58, 'seagull': 53}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n",
      "[2, 3] {'shark': 74, 'turtle': 62, 'dolphin': 58, 'seagull': 53}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n",
      "[0, 1] {'shark': 88, 'turtle': 73, 'dolphin': 78, 'seagull': 86}\n"
     ]
    }
   ],
   "source": [
    "def animal_consumer(parts=[]):\n",
    "    counts = {}  # key=animal, value=count\n",
    "    partitions = [TopicPartition(\"animals\", p) for p in parts]\n",
    "    print(partitions)\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign(partitions)\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if not s.animal in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(parts, counts)\n",
    "\n",
    "threading.Thread(target=animal_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f5d3a-4316-4133-b60a-29308667ec7a",
   "metadata": {},
   "source": [
    "# Spark Streaming Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3e72a2d-41bc-4a79-b817-a6213bc50761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session (with Kafka jar)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"demo\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "124383d7-8d69-4655-b3a0-0fb138c65823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, threading, json\n",
    "\n",
    "def animal_json_producer():\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        #value = Sighting(beach=beach, animal=animal).SerializeToString()\n",
    "        value = bytes(json.dumps({\"beach\": beach, \"animal\": animal}), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value, bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "threading.Thread(target=animal_json_producer).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44db0d2b-1a6a-4dc7-8159-7de8a9d965da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    " spark.read.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", broker)\n",
    " .option(\"subscribe\", \"animals-json\")\n",
    " .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7619f68e-abd9-401e-8f76-abe0047031ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e3ce7e5-91b9-439e-884d-6117a314e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:30:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2de075fc-fd0a-4871-b941-5ca45162e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:31:06 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-20 16:27:03.722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-20 16:27:05.726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-20 16:27:07.728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-11-20 16:27:09.731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-11-20 16:27:10.733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value         topic  \\\n",
       "0  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "1  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "2  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "3  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "4  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2023-11-20 16:27:03.722              0  \n",
       "1          0       1 2023-11-20 16:27:05.726              0  \n",
       "2          0       2 2023-11-20 16:27:07.728              0  \n",
       "3          0       3 2023-11-20 16:27:09.731              0  \n",
       "4          0       4 2023-11-20 16:27:10.733              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a7c606a-125a-41b9-ad02-749f60160760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "356d24b4-c456-4c43-b137-f7af099a9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:35:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   B     B    shark\n",
       "1   C     C  seagull\n",
       "2   B     B  seagull\n",
       "3   B     B   turtle\n",
       "4   B     B  seagull"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "animals = (df\n",
    " .select(\n",
    "     col(\"key\").cast(\"string\"),\n",
    "     col(\"value\").cast(\"string\")\n",
    " )\n",
    " .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    " .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da04cb3c-a47a-4922-877a-90278aa4dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704ae2-a253-44b6-8a8f-9bff42337093",
   "metadata": {},
   "source": [
    "# Let's make it a streaming DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acd5da3a-11da-4676-9ac2-4cedb375fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    " spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", broker)\n",
    " .option(\"subscribe\", \"animals-json\")\n",
    " .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4802bb6b-4613-464d-9717-53c2599e8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "animals = (df\n",
    " .select(\n",
    "     col(\"key\").cast(\"string\"),\n",
    "     col(\"value\").cast(\"string\")\n",
    " )\n",
    " .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    " .select(\"key\", \"value.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3ea9dc1-3860-4714-b467-02dc2acbb11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89803550-61de-4a4f-bbda-cff31bfe8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work for streaming\n",
    "# animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1642ed2-3ba3-4377-8bdb-282bbf2cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark streaming\n",
    "# source => transformations => sink\n",
    "\n",
    "# spark.readStream(????).?????.writeStream(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850896d6-1cc3-49a2-941d-d2864b0f233b",
   "metadata": {},
   "source": [
    "# Shark Alert Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "916d3f8a-dda2-41f8-956c-517cd93c316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:40:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e7532387-90c9-4810-a33c-46925f2d9897. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/20 16:40:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.query.StreamingQuery"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:40:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  D|    D| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "+---+-----+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  B|    B| shark|\n",
      "|  C|    C| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  C|    C| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  G|    G| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  D|    D| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  G|    G| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  A|    A| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = (\n",
    " animals.filter(\"animal = 'shark'\")\n",
    " .writeStream.format(\"console\")\n",
    " .trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"append\")\n",
    ").start()\n",
    "type(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c1cc576-9a1f-451a-83c9-852d4ae3d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()\n",
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cad408-825a-458b-967b-8472655bcba0",
   "metadata": {},
   "source": [
    "# Streaming GROUP BY on animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "831a72e6-00b6-4855-bac0-1c3658f25ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:44:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-abcb8a15-83e8-4f4b-ab7f-7196cb2b52c0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/20 16:44:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/20 16:44:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "23/11/20 16:44:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 27328 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|animal|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|seagull|    5|\n",
      "|  shark|    4|\n",
      "| turtle|    8|\n",
      "|dolphin|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 16:45:02 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 24781 milliseconds\n",
      "[Stage 30:============>                                          (47 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "q = (\n",
    " animals.groupby(\"animal\").count()\n",
    " .writeStream.format(\"console\")\n",
    " .trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"complete\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a6115-182e-4a62-9f47-a8c16ef7cac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
