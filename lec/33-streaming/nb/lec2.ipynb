{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e12da1-6216-41e1-beec-eae2d52c4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m grpc_tools.protoc -I=. --python_out=. animals.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e74c81-d584-4504-9ef5-2c89227d3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import Sighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0f722f-060b-4236-b28c-b8cd86a6bc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beach: \"A\"\n",
       "animal: \"shark\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sighting(animal=\"shark\", beach=\"A\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5daff3-d47f-433f-8933-e074145b1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x01A\\x12\\x05shark'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60cea48-9cd2-4635-a822-b5a8992f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1a5535-36c1-416c-8d70-7493dbf26338",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdec839a-2f6b-4e41-8d79-05271c12fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b0baaa-8ddc-41d3-9091-851009b672bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207f3f33-3edd-4039-9ed2-6a606e854e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='animals-json', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin.create_topics([NewTopic(\"animals\", 4, 1)])   # protobufs\n",
    "admin.create_topics([NewTopic(\"animals-json\", 4, 1)])   # JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf52c987-d4c2-4077-a72b-aada380c81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading\n",
    "\n",
    "def animal_gen():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "        s = Sighting(animal=animal, beach=beach)\n",
    "        producer.send(\"animals\", value=s.SerializeToString(), key=bytes(beach, \"utf-8\"))\n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e042e6-56ec-41c6-bf38-032b7bb899fb",
   "metadata": {},
   "source": [
    "# Streaming Group By (count occurences per beach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7199cf-5c69-40fd-9794-721649a71f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "\n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ba5b05-d29f-42cb-9d35-161a74e149a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bfd1425-ae07-4c09-85d4-e56ad659abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'I': 55, 'B': 44, 'C': 59, 'E': 55, 'D': 58}\n",
      "[2, 3] {'H': 59, 'G': 65, 'A': 53, 'F': 54}\n",
      "[2, 3] {'H': 59, 'G': 65, 'A': 54, 'F': 54}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 59, 'E': 55, 'D': 58}\n",
      "[2, 3] {'H': 59, 'G': 65, 'A': 55, 'F': 54}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 59, 'E': 55, 'D': 58}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 59, 'E': 55, 'D': 59}\n",
      "[2, 3] {'H': 59, 'G': 65, 'A': 55, 'F': 54}\n",
      "[2, 3] {'H': 60, 'G': 65, 'A': 55, 'F': 54}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 59, 'E': 55, 'D': 59}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 60, 'E': 55, 'D': 59}\n",
      "[2, 3] {'H': 60, 'G': 65, 'A': 55, 'F': 54}\n",
      "[2, 3] {'H': 60, 'G': 66, 'A': 55, 'F': 54}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 60, 'E': 55, 'D': 59}\n",
      "[2, 3] {'H': 60, 'G': 66, 'A': 55, 'F': 54}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 60, 'E': 55, 'D': 59}\n",
      "[2, 3] {'H': 60, 'G': 66, 'A': 55, 'F': 55}\n",
      "[2, 3] {'H': 60, 'G': 66, 'A': 56, 'F': 55}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 60, 'E': 55, 'D': 59}\n",
      "[0, 1] {'I': 55, 'B': 44, 'C': 60, 'E': 55, 'D': 60}\n"
     ]
    }
   ],
   "source": [
    "def beach_consumer(partitions=[]):\n",
    "    counts = {}   # key=beach, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.beach in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=beach_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96189610-b7c7-4f65-bd7f-850b3dcc6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'shark': 92, 'dolphin': 76, 'turtle': 74, 'seagull': 73}\n",
      "[2, 3] {'dolphin': 62, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 92, 'dolphin': 76, 'turtle': 74, 'seagull': 74}\n",
      "[2, 3] {'dolphin': 62, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 92, 'dolphin': 77, 'turtle': 74, 'seagull': 74}\n",
      "[2, 3] {'dolphin': 62, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 93, 'dolphin': 77, 'turtle': 74, 'seagull': 74}\n",
      "[2, 3] {'dolphin': 62, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 93, 'dolphin': 77, 'turtle': 74, 'seagull': 74}\n",
      "[0, 1] {'shark': 93, 'dolphin': 77, 'turtle': 74, 'seagull': 75}\n",
      "[2, 3] {'dolphin': 62, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[2, 3] {'dolphin': 63, 'seagull': 59, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 93, 'dolphin': 77, 'turtle': 74, 'seagull': 75}\n",
      "[2, 3] {'dolphin': 63, 'seagull': 60, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 93, 'dolphin': 77, 'turtle': 74, 'seagull': 75}\n",
      "[0, 1] {'shark': 94, 'dolphin': 77, 'turtle': 74, 'seagull': 75}\n",
      "[2, 3] {'dolphin': 63, 'seagull': 60, 'turtle': 65, 'shark': 81}\n",
      "[0, 1] {'shark': 94, 'dolphin': 77, 'turtle': 74, 'seagull': 75}\n",
      "[2, 3] {'dolphin': 63, 'seagull': 60, 'turtle': 65, 'shark': 82}\n",
      "[2, 3] {'dolphin': 64, 'seagull': 60, 'turtle': 65, 'shark': 82}\n"
     ]
    }
   ],
   "source": [
    "def animal_consumer(partitions=[]):\n",
    "    counts = {}   # key=animal, value=count\n",
    "    \n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", p) for p in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    for i in range(10):      # TODO: loop forever\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "\n",
    "                if not s.animal in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(partitions, counts)\n",
    "threading.Thread(target=animal_consumer, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_consumer, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b0bcc-ab55-40a5-a186-4622ea6f49ff",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c206aa-56e1-4aad-b631-85c9cea0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading, json\n",
    "\n",
    "def animal_gen_json():\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "\n",
    "    while True:\n",
    "        beach = random.choice(list(\"ABCDEFGHI\"))\n",
    "        animal = random.choice([\"shark\", \"dolphin\", \"turtle\", \"seagull\"])\n",
    "\n",
    "        value = bytes(json.dumps({\"beach\": beach, \"animal\": animal}), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value=value, key=bytes(beach, \"utf-8\"))\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "threading.Thread(target=animal_gen_json).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8339f693-ab99-4e4e-8040-63033079e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session (with Kafka jar)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"demo\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70f52414-df72-48cb-b851-254a12739ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20e78ed8-3587-4789-9e51-9914158513c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb6f3dd4-3bdc-4f2a-b89d-93e673cf7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 19:54:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-20 19:50:38.664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[73]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-20 19:50:40.667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[73]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-20 19:50:43.672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[67]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-11-20 19:50:46.677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[66]</td>\n",
       "      <td>[123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...</td>\n",
       "      <td>animals-json</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-11-20 19:50:47.678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value         topic  \\\n",
       "0  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "1  [73]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "2  [73]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "3  [67]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "4  [66]  [123, 34, 98, 101, 97, 99, 104, 34, 58, 32, 34...  animals-json   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2023-11-20 19:50:38.664              0  \n",
       "1          0       1 2023-11-20 19:50:40.667              0  \n",
       "2          0       2 2023-11-20 19:50:43.672              0  \n",
       "3          0       3 2023-11-20 19:50:46.677              0  \n",
       "4          0       4 2023-11-20 19:50:47.678              0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73b59cc6-c4c9-4acd-ad85-56ad3809763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa197550-ab9a-439d-a71f-62bab4524c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdce1d54-16e0-459b-aaf5-3ad2bd04ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 19:58:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>beach</th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>dolphin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>seagull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key beach   animal\n",
       "0   C     C  dolphin\n",
       "1   I     I  seagull\n",
       "2   I     I  dolphin\n",
       "3   C     C  dolphin\n",
       "4   B     B  seagull"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c84669-d54e-479b-9ffa-ab4aef9c41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 19:58:31 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e1e80dc-509f-4a6a-8c08-ca8b3d0d6508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c43444-81f8-472f-b81c-1040de0dd58b",
   "metadata": {},
   "source": [
    "# Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ba406-af30-44b2-b0b4-a53f0a95be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source => transformations => sink\n",
    "# streaming_query = spark.readStream(????).????.writeStream(????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed6e1956-a3b3-43ee-bc55-b13ac280e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", broker)\n",
    "    .option(\"subscribe\", \"animals-json\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79e5ed09-6c6f-4b89-b3c9-f046f4de2456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87198d60-46b7-4a41-b038-5b5df8015219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "\n",
    "animals = (\n",
    "    df\n",
    "    .select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "    .select(\"key\", from_json(\"value\", schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")\n",
    ")\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c999e5e-e406-40bc-904c-e9b5c07865fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not supported for streaming\n",
    "# animals.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823de2a-39ac-4070-a021-27aaea5af87b",
   "metadata": {},
   "source": [
    "# Shark Alert App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc05ed91-8f39-473b-861f-907d26ac5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:04:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b4b4a357-7aca-4a59-bd79-9aa6c8006b92. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/20 20:04:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.query.StreamingQuery"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:04:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_query = (\n",
    "    animals\n",
    "    .filter(\"animal='shark'\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"append\")\n",
    ").start()\n",
    "type(streaming_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "660e5254-037c-4c72-970f-30dff81e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f3750-8f0d-4592-a1f2-128ae6900141",
   "metadata": {},
   "source": [
    "# Animal Counter App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a21fbea-6184-4e42-9b13-b6f14aaa46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:07:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7315fa79-c5ed-4c79-b792-7fcf00174494. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/20 20:07:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/20 20:07:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|animal|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:07:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 24977 milliseconds\n",
      "[Stage 27:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|seagull|    9|\n",
      "|  shark|    5|\n",
      "| turtle|    4|\n",
      "|dolphin|    6|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:07:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 21919 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|seagull|   14|\n",
      "|  shark|   13|\n",
      "| turtle|    8|\n",
      "|dolphin|   11|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:08:19 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 19445 milliseconds\n",
      "[Stage 31:===============>                                       (57 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "q = (\n",
    "    animals.groupby(\"animal\").count()\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"complete\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e0d51-782b-43c1-90cb-74329a02671d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
